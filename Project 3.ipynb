{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Project Three: History, or Story?</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T06:44:39.773036Z",
     "start_time": "2021-02-13T06:44:37.949788Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "\n",
    "from IPython.display import display_html\n",
    "\n",
    "plt.style.use(\"dark_background\")\n",
    "plt.figure(figsize=(12,12));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting started, we will want to set a global random seed to ensure reproduceable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T06:44:39.789029Z",
     "start_time": "2021-02-13T06:44:39.774032Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging _y_ variable\n",
    "\n",
    "We have already acquired and done initial clean to our data with the scraper.ipynb and cleaner.ipynb notebooks, respectively.  Here, we have two datasets from their respective subreddits, r/AskHistorians and r/HistoricalWhatIf.  \n",
    "\n",
    "We will begin by identifying which rows belong to the WhatIf sub, before merging the sets into one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T06:44:39.948564Z",
     "start_time": "2021-02-13T06:44:39.791983Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./data/askhistorians_clean.csv')\n",
    "df2 = pd.read_csv('./data/whatif_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T06:44:39.964521Z",
     "start_time": "2021-02-13T06:44:39.950559Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge into one dataset with a single column for target\n",
    "df2['whatif'] = 1\n",
    "df2['whatif'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T06:44:39.979482Z",
     "start_time": "2021-02-13T06:44:39.965519Z"
    }
   },
   "outputs": [],
   "source": [
    "# fill the ask-historians entries as 0\n",
    "df1['whatif'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Cleaning\n",
    "We will then use regular expressions to render the selftext column (our intended X variable) into a more machine-friendly form.  The function below will convert our text to lowercase, strips special characters, punctuation, hyperlinks, and whitespace that might interfere with our modeling.  It also removes any words of two letters or less, which are too common to be sufficiently predictive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T06:44:41.548811Z",
     "start_time": "2021-02-13T06:44:39.980479Z"
    }
   },
   "outputs": [],
   "source": [
    "# clean text\n",
    "\n",
    "# function courtesy of Marta Ghiglioni\n",
    "def cleaner(text):\n",
    "    # Make lowercase\n",
    "    text = text.lower()\n",
    "    # Remove HTML special characters\n",
    "    text = re.sub(r'\\&\\w*;', '', text)\n",
    "    # Remove hyperlinks\n",
    "    text = re.sub(r'https?:\\/\\/.*\\/\\w*', '', text)\n",
    "    # Remove punctuation and split\n",
    "    text = re.sub(r'[' + string.punctuation.replace('@', '') + ']+', ' ', text)\n",
    "    # Remove words with 2 or fewer letters\n",
    "    text = re.sub(r'\\b\\w{1,2}\\b', '', text)\n",
    "    # Remove whitespace (including new line characters)\n",
    "    text = re.sub(r'\\s\\s+', ' ', text)\n",
    "    # Remove characters beyond Basic Multilingual Plane\n",
    "    text = ''.join(c for c in text if c <= '\\uFFFF') \n",
    "    return text\n",
    "\n",
    "df1['selftext'] = df1['selftext'].apply(cleaner)\n",
    "df2['selftext'] = df2['selftext'].apply(cleaner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge\n",
    "\n",
    "Now, we can merge them into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T06:44:41.564725Z",
     "start_time": "2021-02-13T06:44:41.549785Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge the dataframes\n",
    "df = df1.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T06:44:41.580675Z",
     "start_time": "2021-02-13T06:44:41.566712Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop the irrelevant columns\n",
    "df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1', 'subreddit',\n",
    "       'created_utc', 'author', 'num_comments', 'score', 'is_self',\n",
    "       'timestamp'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T06:44:41.596637Z",
     "start_time": "2021-02-13T06:44:41.581672Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df['selftext']\n",
    "y = df['whatif']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T06:44:41.612590Z",
     "start_time": "2021-02-13T06:44:41.598627Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into the training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T06:44:41.628548Z",
     "start_time": "2021-02-13T06:44:41.613588Z"
    }
   },
   "outputs": [],
   "source": [
    "# Baseline accuracy\n",
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TF-IDF to examine word frequency\n",
    "\n",
    "I will be using custom stopwords, in an effort to eliminate common words and zero in on more unusual and predictive terminology.  To do this, I will iterate through the top words in the corpus repeatedly, adding additional words to the custom stopwords with each pass, until I arrive at a top 25 words which all seem reasonably useful for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T06:44:41.644505Z",
     "start_time": "2021-02-13T06:44:41.629544Z"
    }
   },
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T06:44:47.644793Z",
     "start_time": "2021-02-13T06:44:41.645502Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert training data to dataframe\n",
    "X_train_df = pd.DataFrame(tvec.fit_transform(X_train).todense(), \n",
    "                          columns=tvec.get_feature_names())\n",
    "\n",
    "X_train_df = X_train_df.drop(columns=['history', 'what', 'reddit', 'subreddit', 'their', 'any',\n",
    "          'been', 'had', 'from', 'not', 'like', 'with', 'about', 'were',\n",
    "          'did', 'but', 'for', 'they', 'how', 'this', 'what', 'have', 'was',\n",
    "          'that', 'would', 'and', 'after', 'much', 'when', 'them',\n",
    "          'who', 'why', 'could', 'all', 'some', 'his', 'more', 'you', 'are',\n",
    "          'there', 'the', 'where', 'has', 'into', 'which', 'also', 'out', 'does', 'these'])\n",
    "\n",
    "# plot top occuring words\n",
    "X_train_df.sum().sort_values(ascending=False).head(25).plot(kind='barh');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While many of our top 25 are still fairly generic, they are at least meaningful.  We have successfully removed words that are structural rather than informative to be plucked out.\n",
    "\n",
    "In addition, we will remove a few words that would tell us either nothing, or too much, based on the particular dataset.  Namely: \"history\", \"what\", \"if\", \"reddit\" and \"subreddit\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T06:44:47.659752Z",
     "start_time": "2021-02-13T06:44:47.646786Z"
    }
   },
   "outputs": [],
   "source": [
    "# excluding what and if is going to make this a lot harder, but it feels\n",
    "# like the right thing to do in the spirit of the goal I am aiming at.\n",
    "\n",
    "custom = ['history', 'what', 'if', 'reddit', 'subreddit', 'their', 'any',\n",
    "          'been', 'had', 'from', 'not', 'like', 'with', 'about', 'were',\n",
    "          'did', 'but', 'for', 'they', 'how', 'this', 'what', 'have', 'was',\n",
    "          'that', 'would', 'and', 'he', 'after', 'much', 'when', 'them',\n",
    "          'who', 'why', 'could', 'all', 'some', 'his', 'more', 'you', 'are',\n",
    "          'there', 'the', 'where', 'has', 'into', 'which', 'also', 'out', \n",
    "          'does', 'these', 'most', 'many', 'being']\n",
    "combined_words = text.ENGLISH_STOP_WORDS.union(custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selection\n",
    "I selected the following models:\n",
    "\n",
    "1. KNN, to provide a baseline. If more sophisticated modeling techniques cannot beat KNN for accuracy and precision, their sophistication is counterproductive.\n",
    "2. Decision Trees Classifier.  An excellent 'mid-level' model that will bring a more sophisticated technique the the table, yet still run quickly and be highly interpretable.  Will require tuning to avoid overfit.\n",
    "3. Random Forest Classifier.  An improvement upon decision trees: by selecting a random subset of features at each split, we decrease decision trees' tendency toward high variance.  Unfortunately, this will be less interpretable.\n",
    "4. Support Vector Classifier.  A very effective classifier for high dimensional data (such as NLP data).  Expected to be the most accurate model used.  Black box, not interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T06:45:02.842005Z",
     "start_time": "2021-02-13T06:44:47.660749Z"
    }
   },
   "outputs": [],
   "source": [
    "knn_cvec = CountVectorizer(analyzer = \"word\",\n",
    "                       preprocessor = None,\n",
    "                       stop_words = combined_words, \n",
    "                       max_features = 100, \n",
    "                       ngram_range = (1, 4)\n",
    "                      )\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn_sc = StandardScaler(with_mean=False)\n",
    "\n",
    "knn_pipe = Pipeline([\n",
    "    ('cvec', knn_cvec),\n",
    "    ('sc', knn_sc),\n",
    "    ('knn', knn)\n",
    "])\n",
    "\n",
    "knn_pipe.fit(X_train, y_train)\n",
    "\n",
    "knn_train = knn_pipe.score(X_train, y_train)\n",
    "knn_test = knn_pipe.score(X_test, y_test)\n",
    "print(f'KNN, score on training set: {knn_train}, score on test set: {knn_test}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T06:45:06.612054Z",
     "start_time": "2021-02-13T06:45:02.843540Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(knn_pipe, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T06:45:10.225808Z",
     "start_time": "2021-02-13T06:45:06.614048Z"
    }
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=12, min_samples_split=180,\n",
    "                            min_samples_leaf=60,\n",
    "                            random_state=42)\n",
    "p_stemmer = PorterStemmer()\n",
    "dt_cvec = CountVectorizer(analyzer=\"word\",\n",
    "                          preprocessor=None,\n",
    "                          stop_words=combined_words,\n",
    "                          max_features=5,\n",
    "                          max_df=.90,\n",
    "                          min_df=.10,\n",
    "                          ngram_range=(1,2)\n",
    "                          )\n",
    "dt_pipe = Pipeline([\n",
    "    ('cvec', dt_cvec),\n",
    "    ('dt', dt)\n",
    "])\n",
    "\n",
    "dt_pipe.fit(X_train, y_train)\n",
    "\n",
    "dt_train = dt_pipe.score(X_train, y_train)\n",
    "dt_test = dt_pipe.score(X_test, y_test)\n",
    "\n",
    "print(f'Decision Trees, Score on training set: {dt_train}, score on test set: {dt_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T06:45:10.240767Z",
     "start_time": "2021-02-13T06:45:10.226804Z"
    }
   },
   "outputs": [],
   "source": [
    "# Possible gridsearch to try to make DT converge properly.\n",
    "#\n",
    "# grid = GridSearchCV(estimator=dt,\n",
    "#                     param_grid={'max_depth': [2,3,5,7],\n",
    "#                                'min_samples_split': [5,10,15,20],\n",
    "#                                'min_samples_leaf': [2,3,4,5,6],\n",
    "#                                'ccp_alpha': [0,0.001, 0.01, 0.1, 1, 10]},\n",
    "#                     cv = 5,\n",
    "#                     verbose = 1)\n",
    "\n",
    "# grid.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T06:45:10.896056Z",
     "start_time": "2021-02-13T06:45:10.241765Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(dt_pipe, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T06:45:21.983851Z",
     "start_time": "2021-02-13T06:45:10.897054Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_cvec = CountVectorizer(analyzer = \"word\",\n",
    "                       preprocessor = None,\n",
    "                       stop_words = combined_words, \n",
    "                       max_features = 1000, \n",
    "                       ngram_range = (1, 4)\n",
    "                      )\n",
    "                    \n",
    "rf = RandomForestClassifier(random_state = 42, n_estimators = 25)\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "    ('cvec', rf_cvec),\n",
    "    ('rf', rf)\n",
    "])\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "rf_train = rf_pipe.score(X_train, y_train)\n",
    "rf_test = rf_pipe.score(X_test, y_test)\n",
    "print(f'Random Forest, score on training set: {rf_train}, score on test set: {rf_test}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T06:45:23.264859Z",
     "start_time": "2021-02-13T06:45:21.985845Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(rf_pipe, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T07:20:39.081820Z",
     "start_time": "2021-02-13T06:45:23.265856Z"
    }
   },
   "outputs": [],
   "source": [
    "svc_cvec = CountVectorizer(analyzer = \"word\",\n",
    "                       preprocessor = None,\n",
    "                       stop_words = combined_words, \n",
    "                       max_features = 2000, \n",
    "                       ngram_range = (1, 1),\n",
    "                      )\n",
    "\n",
    "pgrid = {\"svc__C\": np.linspace(0.01, 1, 20)}\n",
    "\n",
    "svc = SVC(max_iter=7000, tol=0.1) # model object\n",
    "\n",
    "svc_sc = StandardScaler(with_mean=False)\n",
    "\n",
    "svc_pipe = Pipeline([\n",
    "    ('cvec', svc_cvec),\n",
    "    ('sc', svc_sc),\n",
    "    ('svc', svc)\n",
    "])\n",
    "\n",
    "svc_grid = GridSearchCV(svc_pipe,\n",
    "                        pgrid,\n",
    "                        cv = 5)\n",
    "\n",
    "svc_grid.fit(X_train, y_train)\n",
    "svc_train = svc_grid.score(X_train, y_train)\n",
    "svc_test = svc_grid.score(X_test, y_test)\n",
    "print(f'SVC, score on training set: {svc_train}, score on test set: {svc_test}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T07:20:58.093625Z",
     "start_time": "2021-02-13T07:20:39.083815Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(svc_grid, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T07:21:17.617440Z",
     "start_time": "2021-02-13T07:20:58.095620Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = svc_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning\n",
    "With results in, I selected the best-performing models for additional tuning.  I will set up a gridsearch to find the best parameters for the Random Forest and SVM classifiers before declaring one of the models victorious.\n",
    "\n",
    "I have commented the search cells in the interest of time, but the variety of parameters I tried searching is reflected here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T07:32:13.049819Z",
     "start_time": "2021-02-13T07:32:13.037346Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# rf_params = {'cvec__analyzer':  ['word'],\n",
    "#              'cvec__stop_words': [combined_words],\n",
    "#              'cvec__max_features': [1000],\n",
    "#              'cvec__ngram_range': [(1, 4)],\n",
    "#              'rf__random_state': [42],\n",
    "#              'rf__n_estimators': [5, 15, 25],\n",
    "#              'rf__max_depth': [3, 5, 8],\n",
    "#              'rf__max_features': [5, 10, 15, 20],\n",
    "#              'rf__max_leaf_nodes': [5, 8, 12],\n",
    "#              'rf__min_samples_leaf': [9, 15, 25],\n",
    "#              'rf__bootstrap': [True, False]\n",
    "#             }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T07:32:16.886085Z",
     "start_time": "2021-02-13T07:32:16.774815Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GridSearchCV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5c0960df7606>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m gs1 = GridSearchCV(rf_pipe,\n\u001b[0m\u001b[0;32m      2\u001b[0m                    \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrf_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                    cv=5)\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgs1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GridSearchCV' is not defined"
     ]
    }
   ],
   "source": [
    "gs1 = GridSearchCV(rf_pipe,\n",
    "                   param_grid=rf_params,\n",
    "                   cv=5)\n",
    "\n",
    "gs1.fit(X_train, y_train)\n",
    "print(gs1.best_score_)\n",
    "gs1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T07:32:17.314179Z",
     "start_time": "2021-02-13T07:32:17.308194Z"
    }
   },
   "outputs": [],
   "source": [
    "# svc_params = {\n",
    "#     'max_iter': [3000, 5000, 7000],\n",
    "#     'tol': [0.001, 0.01, 0.1],\n",
    "#     'C': [0.1, 0.5, 1],\n",
    "#     'class_weight': ['balanced', None]\n",
    "# }\n",
    "\n",
    "# gs2 = GridSearchCV(svc_pipe,\n",
    "#                   param_grid=svc_params,\n",
    "#                   cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T07:32:17.707713Z",
     "start_time": "2021-02-13T07:32:17.692753Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gs2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-88afa1ed4927>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgs2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgs2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mgs2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gs2' is not defined"
     ]
    }
   ],
   "source": [
    "gs2.fit(X_train, y_train)\n",
    "print(gs2.best_score_)\n",
    "gs2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T07:21:18.163940Z",
     "start_time": "2021-02-13T06:44:43.755Z"
    }
   },
   "outputs": [],
   "source": [
    "# # notes for future improvements\n",
    "\n",
    "# most common ten words\n",
    "\n",
    "# Tiffydiff\n",
    "\n",
    "# Lemma/stem\n",
    "\n",
    "# plot ROC curve\n",
    "\n",
    "# explain choice of measurement\n",
    "\n",
    "# pull out misclassifications to examine\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of Measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using Accuracy as our target metric, to gauge the overall feasibility of predicting our target category (fact / fiction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a maximum accuracy of .83, it is too soon to say this model can be used to predict fiction reliably in real world scenarios, when the stakes are high.\n",
    "\n",
    "Unfortunately gridsearching for better-tuned parameters had to be cut short.  The above accuracy does not reflect a well-tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will not be seen how well this technique generalizes from the field of history to another, such as current politics vs. propaganda.\n",
    "\n",
    "However, the goal was to provide proof of concept and assess the feasibility of this as a practical application of machine learning, and we have very much achieved that.  Accuracy of .83 is a more than reasonable starting benchmark, and can surely be improved into something much more robust."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
