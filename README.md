# History, or Story?

## ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Project 3: Web APIs & NLP

### Executive Summary & Problem Statement

In this day and age of rampant misinformation, and especially in the era to come of [deepfake text](https://www.wired.com/story/ai-generated-text-is-the-scariest-deepfake-of-all/) generated by AI, distinguishing fact and fiction is both more important, and more difficult, than ever.

To that end, this project seeks to provide proof-of-concept on using classification models to distinguish between two similar sets of textual data, one fictive, and one factual.  To provide a challenge for our model, I have selected the History-related subreddits [r/AskHistorians](https://www.reddit.com/r/AskHistorians/) and [r/HistoricalWhatIf](https://www.reddit.com/r/HistoricalWhatIf/).

With the modeling techniques and insights gained from addressing this problem, we hope to inform the design of more sophisticated machine learning methods to identify and flag political misinformation, automate fact-checking, and/or classify rhetoric vs. neutrality in reporting.  Our goal is to provide proof of concept and lay a groundwork for what is likely to be a crucial area in the immediate future, both in terms of social value and stakeholder value.  As more and more data is produced every day, it is crucial to have a means of verifying the factual integrity of that data.

### Project Description

Using webscraping via the PushShift API and various Natural Language Processing tools and techniques, train a classifier to determine which of two subreddits a given post came from, based on its text content.


### Methodology and Model Selection

With this project, I aim to identify a classification that stretches the capabilities of NLP techniques.  Narrowing our sampleset to single topic - history - allows us to come closer to isolating the distinction we ultimately want to pick out: distinguishing factual information from speculation.  

From this starting point, I explored several of the more popular options in the overall topic on Reddit, eventually selecting two with similar levels of activity, years of past content, tone, and overall sum of content: r/HistoricalWhatIf and r/AskHistorians.

With data gathered, it will also be necessary to identify and clean other 'giveaway' indicators that could allow the model to predict based on externalities.  The words "what if" are the most glaring example, but EDA should turn up others which will in turn be excluded from the featureset.

Lastly, I will apply several distinct modeling methods to the problem.  While interpretability is not a priority in the end product, I hope to use one or more interpretable models in the process, in order to gain insights that will in turn inform the modeling, particularly in terms of feature selection and/or engineering.  The end goal is to have the most accurate and precise model possible (F1 score will be the target metric), while knowing enough about how the model achieves it's predictions to be confident that it is the text content itself which is being analyzed.


### Conclusions & Recommendations

With a maximum accuracy of .83, it is too soon to say this model can be used to predict fiction reliably in real world scenarios, when the stakes are high.

It will not be seen how well this technique generalizes from the field of history to another, such as current politics vs. propaganda.

However, the goal was to provide proof of concept and assess the feasibility of this as a practical application of machine learning, and we have very much achieved that.  Accuracy of .83 is a more than reasonable starting benchmark, and can surely be improved into something much more robust.
